"""
code ho√†n ch·ªânh
"""

# import base64
# import json
# import requests

# with open("/home/tiennv/phucth/medical/data_test/data/testcase_medical/image_test/test4.png", "rb") as f:
#     image_b64 = base64.b64encode(f.read()).decode("utf-8")

# payload = {
#     "prompt": "USER: <image>\n·∫¢nh ch·ª•p X-ray PA (Ch·ª•p Xquang tim ph·ªïi th·∫≥ng) b·ªánh nh√¢n nam, 84 tu·ªïi. Cho bi·∫øt b·ªánh nh√¢n b·ªã g√¨?.\nASSISTANT:",
#     "max_tokens": 256,
#     "temperature": 0.1,
#     "multi_modal_data": json.dumps({"image": image_b64})
# }

# res = requests.post(
#     "http://localhost:8000/v2/models/THP2903clara_multiturn/generate",
#     headers={"Content-Type": "application/json"},
#     data=json.dumps(payload)
# )

# print(res.json())
import base64
import json
import requests
from transformers import AutoProcessor
from PIL import Image

# Load processor ƒë·ªÉ d√πng apply_chat_template
model_name = "/home/tiennv/phucth/medical/model/clara_multiturn"
processor = AutoProcessor.from_pretrained(model_name)

# Load image v√† encode base64
image_path = "/home/tiennv/phucth/medical/data_test/data/testcase_medical/image_test/test4.png"
with open(image_path, "rb") as f:
    image_b64 = base64.b64encode(f.read()).decode("utf-8")

image_pil = Image.open(image_path).convert("RGB").resize((448, 448))

# Build conversation like you did with transformers
conversation = [
    {"role": "system", "content": "You are a helpful assistant."},
    {
        "role": "user",
        "content": [
            {"type": "image", "image": image_pil},
            {"type": "text", "text": "·∫¢nh ch·ª•p X-ray PA (Ch·ª•p Xquang tim ph·ªïi th·∫≥ng) b·ªánh nh√¢n nam, 84 tu·ªïi. Cho bi·∫øt b·ªánh nh√¢n b·ªã g√¨?"}
        ]
    }
]

# Convert to prompt
prompt = processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)

# Payload to Triton
payload = {
    "prompt": prompt,
    "max_tokens": 512,
    "temperature": 0.1,
    "multi_modal_data": json.dumps({"image": image_b64})
}

res = requests.post(
    "http://localhost:8000/v2/models/THP2903clara_multiturn/generate",  # l∆∞u √Ω t√™n model ph·∫£i kh·ªõp
    headers={"Content-Type": "application/json"},
    data=json.dumps(payload)
)

print(res.json())





"""
test v√† check prompt

"""
# from transformers import AutoProcessor
# from PIL import Image

# model_name = "/home/tiennv/phucth/medical/model/clara_multiturn"
# processor = AutoProcessor.from_pretrained(model_name)

# image = Image.open("/home/tiennv/phucth/medical/data_test/data/testcase_medical/image_test/test4.png").convert("RGB").resize((448, 448))

# conversation = [
#     {"role": "user", "content": [{"type": "text", "text": "·∫¢nh ch·ª•p X-ray PA b·ªánh nh√¢n nam, 84 tu·ªïi. Cho bi·∫øt b·ªánh nh√¢n b·ªã g√¨?"}, {"type": "image", "image": image}]}
# ]

# prompt = processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)
# print("Prompt:", prompt)





# import base64
# import json
# import requests

# # === B∆∞·ªõc 1: Load ·∫£nh v√† encode base64 ===
# with open("/home/tiennv/phucth/medical/data_test/data/testcase_medical/image_test/test4.png", "rb") as f:
#     image_b64 = base64.b64encode(f.read()).decode("utf-8")

# # === B∆∞·ªõc 2: T·∫°o bi·∫øn l∆∞u h·ªôi tho·∫°i ===
# chat_history = []

# # === B∆∞·ªõc 3: H√†m g·ª≠i c√¢u h·ªèi ===
# def ask(question, image_b64=None):
#     global chat_history

#     # N·∫øu l√† c√¢u ƒë·∫ßu ti√™n ‚Üí th√™m image
#     if len(chat_history) == 0:
#         chat_history.append(f"USER: <image>\n{question}\nASSISTANT:")
#     else:
#         chat_history.append(f"USER: {question}\nASSISTANT:")

#     # Gh√©p prompt t·ª´ to√†n b·ªô l·ªãch s·ª≠
#     full_prompt = "\n".join(chat_history)

#     payload = {
#         "prompt": full_prompt,
#         "max_tokens": 256,
#         "temperature": 0.1,
#     }

#     if image_b64 is not None and len(chat_history) == 1:
#         payload["multi_modal_data"] = json.dumps({"image": image_b64})

#     # G·ª≠i request
#     res = requests.post(
#         "http://localhost:8000/v2/models/THP2903Qwen2vl_instruct_medical_2/generate",
#         headers={"Content-Type": "application/json"},
#         data=json.dumps(payload)
#     )

#     # Check l·ªói HTTP
#     if res.status_code != 200:
#         print("‚ùå HTTP Error:", res.status_code)
#         print("üìÑ Response:", res.text)
#         return "Error"

#     try:
#         res_json = res.json()
#         response = res_json.get("text", "").strip()
#     except Exception as e:
#         print("‚ùå JSON decode error:", e)
#         print("üìÑ Raw response:", res.text)
#         return "Error"

#     chat_history.append(response)
#     return response


# # === B∆∞·ªõc 4: G·ªçi l·∫ßn l∆∞·ª£t 2 c√¢u h·ªèi ===
# q1 = "·∫¢nh ch·ª•p X-ray PA (Ch·ª•p Xquang tim ph·ªïi th·∫≥ng) b·ªánh nh√¢n nam, 84 tu·ªïi. Cho bi·∫øt b·ªánh nh√¢n b·ªã g√¨?"
# a1 = ask(q1, image_b64=image_b64)

# q2 = "C√≥ d·∫•u hi·ªáu t·ªïn th∆∞∆°ng lan t·ªèa ·ªü ph·ªïi kh√¥ng?"
# a2 = ask(q2)

# # === B∆∞·ªõc 5: In to√†n b·ªô h·ªôi tho·∫°i ===
# print("Q1:", q1)
# print("A1:", a1)
# print("Q2:", q2)
# print("A2:", a2)
