import gradio as gr
from clara_infer import multiturn_infer as clara_infer
# from clara_infer import singleturn_infer as clara_infer
# from clara_lora import multiturn_infer as clara_infer
# from gemini_api import gemini_multiturn_infer as gemini_infer
# from gpt_api import gpt_multiturn_infer as gpt_infer
import gradio as gr
import torch
from transformers import AutoModelForVision2Seq, AutoProcessor
from PIL import Image
from io import BytesIO
import base64
from huggingface_hub import login
from check_xray import is_chest_xray
# ==== Kh·ªüi t·∫°o bi·∫øn global ====



# ƒêƒÉng nh·∫≠p Hugging Face (n·∫øu c·∫ßn)
login(token="")

# Thi·∫øt l·∫≠p thi·∫øt b·ªã
device = "cuda" if torch.cuda.is_available() else "cpu"

# Danh s√°ch model c√≥ th·ªÉ ch·ªçn
MODEL_INFOS = {
    "Clara": {
        "model_id": "THP2903/Qwen2-VL-7B-multi-137k_full_maed_v3",
        "processor_id": "THP2903/Qwen2-VL-7B-multi-137k_full_maed_v3"
        # "model_id": "THP2903/Qwen2vl_instruct_medical_2",
        # "processor_id": "THP2903/Qwen2vl_instruct_medical_2"
    },
    "Clara-mini": {
        "model_id": "THP2903/Qwen2-VL-7B-multi-137k_full_maed_v3",
        "processor_id": "THP2903/Qwen2-VL-7B-multi-137k_full_maed_v3"
    }
}

import torch
from transformers import AutoModelForVision2Seq, AutoProcessor
from PIL import Image
from io import BytesIO
import base64
from huggingface_hub import login
from fastapi import FastAPI, UploadFile, File, Form
from fastapi.responses import JSONResponse
from PIL import Image
import io
import torch

app = FastAPI(title="Qwen2VL Medical API")
# ƒêƒÉng nh·∫≠p Hugging Face (n·∫øu c·∫ßn)
login(token="")


# Thi·∫øt l·∫≠p thi·∫øt b·ªã
device = "cuda" if torch.cuda.is_available() else "cpu"
# device = "cpu"  # Ch·∫°y tr√™n CPU n·∫øu kh√¥ng c√≥ GPU
# Danh s√°ch model c√≥ th·ªÉ ch·ªçn
# MODEL_INFOS = {
#     "Clara": {
#         "model_id": "THP2903/Qwen2-VL-7B-multi-137k_full_maed",
#         "processor_id": "Qwen/Qwen2-VL-7B-Instruct"
#         # "model_id": "THP2903/Qwen2vl_instruct_medical_2",
#         # "processor_id": "THP2903/Qwen2vl_instruct_medical_2"
#     },
#     "Clara-mini": {
#         "model_id": "THP2903/Qwen2vl_7b_instruct_medical_multiturn_full",
#         "processor_id": "THP2903/Qwen2vl_7b_instruct_medical_multiturn_full"
#     }
# }
from huggingface_hub import hf_hub_download
MAE_CKPT = hf_hub_download(
    repo_id="THPBi/mae_med",
    filename="loss=0.02.ckpt",
    token="",
    subfolder="files/output_ptln/sample-epoch=060-valid"
)
# MODEL_INFOS = {
#     "Clara": {
#         "model_id": "THP2903/Qwen2-VL-7B-multi-137k_full_maed_v3",
#         "processor_id": "Qwen/Qwen2-VL-7B-Instruct"
#     },
#     "Clara-custom": {
#         "base_model": "Qwen/Qwen2-VL-7B-Instruct",
#         "lora_path": "THP2903/Clara-7B-multi-137k-mae",
#         "processor_id": "Qwen/Qwen2-VL-7B-Instruct",
#         "mae_ckpt": MAE_CKPT,
#     }
# }

# ====== H√†m ki·ªÉm tra ·∫£nh X-quang tr∆∞·ªõc khi g·ªçi model ======
# def safe_clara_infer(image, model_selector, state, sex, age, view):
#     if image is None:
#         return [[None, "‚ùå B·∫°n c·∫ßn upload ·∫£nh X-quang."]], state, None
#     if not is_chest_xray(image):
#         return [[None, "‚ùå ƒê√¢y kh√¥ng ph·∫£i ·∫£nh X-quang ng·ª±c."]], state, None
#     return clara_infer(image, model_selector, state, sex, age, view)
# def safe_clara_infer(image, model_selector, state, sex, age, view):
#     if image is None:
#         return [[None, "‚ùå B·∫°n c·∫ßn upload ·∫£nh X-quang."]], state

#     if not is_chest_xray(image):
#         return [[None, "‚ùå ƒê√¢y kh√¥ng ph·∫£i ·∫£nh X-quang ng·ª±c."]], state

#     # ƒê·∫£m b·∫£o c√≥ "llm_history" trong state
#     if "llm_history" not in state:
#         state["llm_history"] = []

#     # Ghi log v√†o LLM history
#     state["llm_history"].append({"role": "user", "content": "Ph√¢n t√≠ch ·∫£nh X-quang"})

#     # G·ªçi model ch√≠nh
#     response, state, _ = clara_infer(image, model_selector, state, sex, age, view)

#     # Ghi l·∫°i k·∫øt qu·∫£ v√†o LLM history
#     result_text = response[0][1] if response else "Kh√¥ng c√≥ k·∫øt qu·∫£"
#     state["llm_history"].append({"role": "assistant", "content": result_text})

#     return response, state

# def safe_clara_infer(image, model_selector, state, sex, age, view):
#     if image is None:
#         return [[None, "‚ùå B·∫°n c·∫ßn upload ·∫£nh X-quang."]], state

#     if not is_chest_xray(image):
#         return [[None, "‚ùå ƒê√¢y kh√¥ng ph·∫£i ·∫£nh X-quang ng·ª±c."]], state

#     # ‚úÖ √âp l·∫°i state v·ªÅ dict n·∫øu b·ªã truy·ªÅn sai ki·ªÉu
#     if not isinstance(state, dict):
#         state = {"qwen_history": [], "last_image": None, "llm_history": []}
#     elif "llm_history" not in state:
#         state["llm_history"] = []

#     # üß† Log y√™u c·∫ßu ph√¢n t√≠ch ·∫£nh v√†o l·ªãch s·ª≠ chat
#     state["llm_history"].append({"role": "user", "content": "Ph√¢n t√≠ch ·∫£nh X-quang"})

#     # ü©ª G·ªçi model ch√≠nh Clara
#     response, state, _ = clara_infer(image, model_selector, state, sex, age, view)

#     # üí¨ Ghi l·∫°i ph·∫£n h·ªìi t·ª´ m√¥ h√¨nh v√†o l·ªãch s·ª≠ chat
#     result_text = response[0][1] if response else "Kh√¥ng c√≥ k·∫øt qu·∫£"
#     state["llm_history"].append({"role": "assistant", "content": result_text})

#     return response, state

def safe_clara_infer(image, model_selector, state, sex, age, view):
    if not isinstance(state, dict):
        state = {"qwen_history": [], "last_image": None, "llm_history": []}

    if image is None:
        return [[None, "‚ùå B·∫°n c·∫ßn upload ·∫£nh X-quang."]], state

    if not is_chest_xray(image):
        return [[None, "‚ùå ƒê√¢y kh√¥ng ph·∫£i ·∫£nh X-quang ng·ª±c."]], state

    # Ghi log v√†o LLM history
    state["llm_history"].append({"role": "user", "content": "Ph√¢n t√≠ch ·∫£nh X-quang"})

    # G·ªçi model ch√≠nh
    response, state, _ = clara_infer(image, model_selector, state, sex, age, view)

    # Ghi l·∫°i k·∫øt qu·∫£ v√†o LLM history
    result_text = response[0][1] if response else "Kh√¥ng c√≥ k·∫øt qu·∫£"
    state["llm_history"].append({"role": "assistant", "content": result_text})

    return response, state



# def safe_gemini_infer(image, sex, age, view, state):
#     if image is None:
#         return [[None, "‚ùå B·∫°n c·∫ßn upload ·∫£nh X-quang."]], state, None
#     if not is_chest_xray(image):
#         return [[None, "‚ùå ƒê√¢y kh√¥ng ph·∫£i ·∫£nh X-quang ng·ª±c."]], state, None
#     return gemini_infer(image, sex, age, view, state)

# def safe_gpt_infer(image, sex, age, view, state):
#     if image is None:
#         return [[None, "‚ùå B·∫°n c·∫ßn upload ·∫£nh X-quang."]], state, None
#     if not is_chest_xray(image):
#         return [[None, "‚ùå ƒê√¢y kh√¥ng ph·∫£i ·∫£nh X-quang ng·ª±c."]], state, None
#     return gpt_infer(image, sex, age, view, state)





# ===== Gradio UI =====
# with gr.Blocks() as demo:
#     gr.Markdown("## Tr·ª£ l√Ω Clara - Ph√¢n t√≠ch ·∫£nh X-quang ng·ª±c")

#     # INPUT CHUNG
#     with gr.Row():
#         sex_input = gr.Dropdown(choices=["Nam", "N·ªØ"], label="Gi·ªõi t√≠nh")
#         age_input = gr.Textbox(label="Tu·ªïi", placeholder="VD: 45")
#         view_input = gr.Dropdown(choices=["PA", "Lateral", "AP"], label="G√≥c ch·ª•p", value="PA")
#     image_input = gr.Image(type="pil", label="Upload ·∫£nh X-quang")
    
#     # State l∆∞u ·∫£nh v√† l·ªãch s·ª≠ model
#     shared_image = gr.State(None)         # l∆∞u ·∫£nh d√πng chung cho 2 tab
#     state_clara = gr.State({"qwen_history": [], "last_image": None})
#     state_gemini = gr.State([])
#     state_gpt = gr.State([])

#     # Khi ng∆∞·ªùi d√πng upload ·∫£nh ‚Üí c·∫≠p nh·∫≠t shared_image
#     def update_shared_image(image):
#         return image

#     image_input.change(fn=update_shared_image, inputs=image_input, outputs=shared_image)
    
#     # TABS
#         # === C·ªòT TR√ÅI: INPUT ===
        
#     with gr.Tabs():
#         with gr.Tab("Clara (Qwen)"):
#             clara_model_selector = gr.Dropdown(choices=["Clara", "Clara-mini"], value="Clara", label="Ch·ªçn model Clara")
#             submit_c = gr.Button("Ph√¢n t√≠ch v·ªõi Clara")
#             chatbot_c = gr.Chatbot(label="K·∫øt qu·∫£ t·ª´ Clara", height=500)

#             # submit_c.click(
#             #     fn=clara_infer,
#             #     inputs=[shared_image, clara_model_selector, state_clara, sex_input, age_input, view_input],
#             #     outputs=[chatbot_c, state_clara, image_input]  # v·∫´n reset image n·∫øu b·∫°n mu·ªën, c√≥ th·ªÉ b·ªè n·∫øu kh√¥ng
#             # )
#             submit_c.click(
#                 fn=safe_clara_infer,
#                 inputs=[shared_image, clara_model_selector, state_clara, sex_input, age_input, view_input],
#                 outputs=[chatbot_c, state_clara, image_input]
#             )

#         with gr.Tab("Gemini"):
#             submit_g = gr.Button("Ph√¢n t√≠ch v·ªõi Gemini")
#             output_g = gr.Textbox(label="K·∫øt qu·∫£ t·ª´ Gemini", lines=10)

#             # submit_g.click(
#             #     fn=gemini_infer,
#             #     inputs=[shared_image, sex_input, age_input, view_input, state_gemini],
#             #     outputs=[output_g, state_gemini]
#             # )
#             submit_g.click(
#                 fn=safe_gemini_infer,
#                 inputs=[shared_image, sex_input, age_input, view_input, state_gemini],
#                 outputs=[output_g, state_gemini]
#             )

            
#         with gr.Tab("GPT-4o"):
#             submit_g = gr.Button("Ph√¢n t√≠ch v·ªõi GPT-4o")
#             output_g = gr.Textbox(label="K·∫øt qu·∫£ t·ª´ GPT-4o", lines=10)

#             # submit_g.click(
#             #     fn=gpt_infer,
#             #     inputs=[shared_image, sex_input, age_input, view_input, state_gpt],
#             #     outputs=[output_g, state_gpt]
#             # )
            
#             submit_g.click(
#                 fn=safe_gpt_infer,
#                 inputs=[shared_image, sex_input, age_input, view_input, state_gpt],
#                 outputs=[output_g, state_gpt]
#             )

#         # N√öT RESET TO√ÄN B·ªò
#     reset_btn = gr.Button("üóëÔ∏è Reset t·∫•t c·∫£", variant="stop")

#     def reset_all():
#         return (
#             None,  # image_input
#             None,  # sex_input
#             "",    # age_input
#             "PA",  # view_input
#             [],    # state_clara
#             [],    # state_gemini
#             [],    # state_gpt
#             None,  # clara chatbot
#             "",    # gemini output
#             ""     # gpt output
#         )

#     reset_btn.click(
#         fn=reset_all,
#         inputs=[],
#         outputs=[
#             image_input,
#             sex_input,
#             age_input,
#             view_input,
#             state_clara,
#             state_gemini,
#             state_gpt,
#             chatbot_c,
#             output_g,
#             output_g  # GPT v√† Gemini d√πng chung output_g, n·∫øu kh√°c th√¨ s·ª≠a l·∫°i
#         ]
#     )
# Load model/tokenizer 1 l·∫ßn


# ****************************************************************

# def clean_qwen_response(text: str) -> str:
#     # C·∫Øt ph·∫ßn n·∫±m gi·ªØa <|im_start|>assistant ... <|im_end|>
#     if "<|im_start|>assistant" in text:
#         text = text.split("<|im_start|>assistant")[-1]
#     if "<|im_end|>" in text:
#         text = text.split("<|im_end|>")[0]
#     return text.strip()
# # def truncate_response(text, max_lines=5):
# #     lines = text.split("\n")
# #     truncated = "\n".join(lines[:max_lines])
# #     if len(lines) > max_lines:
# #         truncated += "\nüëâ [C√¢u tr·∫£ l·ªùi d√†i ‚Äì xem th√™m b√™n d∆∞·ªõi]"
# #     return truncated


# SYSTEM_PROMPT = 'You are a language model that answers medical questions using the provided context. Use step-by-step reasoning with **thought**, **action**, and **observation**. At the end, summarize your reasoning inside <think>...</think> and provide the final answer in Vietnamese. Always reply entirely in Vietnamese using proper medical terms.'
# # device_map = "cuda" if torch.cuda.is_available() else "cpu"
# from transformers import AutoModelForCausalLM, AutoTokenizer
# llm_model = AutoModelForCausalLM.from_pretrained(
#     "ChaosAiVision/DeepSeek-R1-0528-Qwen3-8B-vi-sft-medical-9k",
#     torch_dtype="auto",
#     device_map="auto"
# )
# llm_tokenizer = AutoTokenizer.from_pretrained('ChaosAiVision/DeepSeek-R1-0528-Qwen3-8B-vi-sft-medical-9k') # Qwen/Qwen3-1.7B
# def llm_chat_infer(user_input, history):
#     history = history or []
#     chat_list = [{"role": "system", "content": SYSTEM_PROMPT}] + history
#     chat_list.append({"role": "user", "content": user_input})

#     # Build prompt
#     text_prompt = llm_tokenizer.apply_chat_template(
#         chat_list,
#         tokenize=False,
#         add_generation_prompt=True,
#         # enable_thinking=True
#     )

#     inputs = llm_tokenizer(text_prompt, return_tensors="pt").to("cuda")
#     outputs = llm_model.generate(
#         input_ids=inputs.input_ids,
#         attention_mask=inputs.attention_mask,
#         max_new_tokens=1024,
#         temperature=0.7,
#         top_p=0.9,
#         do_sample=True
#     )
#     response_text = llm_tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
#     response_text = clean_qwen_response(response_text)

#     history.append({"role": "user", "content": user_input})
#     history.append({"role": "assistant", "content": response_text})
#     # print(">>> Prompt:", text_prompt)
#     # print(">>> LLM response:", response_text)

#     return response_text, history
# ****************************************************************

# def llm_gr_chat(user_input, state):
#     if not isinstance(state, dict):
#         state = {"qwen_history": [], "last_image": None, "llm_history": []}

#     history = state.get("llm_history", [])
#     response, updated_history = llm_chat_infer(user_input, history)
#     state["llm_history"] = updated_history

#     # Chuy·ªÉn th√†nh d·∫°ng (user, assistant) ƒë·ªÉ Chatbot hi·ªÉn th·ªã
#     chat_ui = []
#     for i in range(1, len(updated_history), 2):  # t·ª´ng c·∫∑p user - assistant
#         user_msg = updated_history[i - 1]["content"]
#         assistant_msg = updated_history[i]["content"]
#         chat_ui.append((user_msg, assistant_msg))

#     return chat_ui, state

# def llm_gr_chat(user_input, state):
#     if not isinstance(state, dict):
#         state = {"qwen_history": [], "last_image": None, "llm_history": []}

#     history = state.get("llm_history", [])
#     response, updated_history = llm_chat_infer(user_input, history)
#     state["llm_history"] = updated_history

#     # L·∫•y to√†n b·ªô h·ªôi tho·∫°i ƒëang c√≥
#     chat_ui = []
#     for i in range(1, len(updated_history), 2):
#         u = updated_history[i - 1].get("content", "")
#         a = updated_history[i].get("content", "")
#         chat_ui.append((u, a))

#     # ‚úÖ C·∫©n th·∫≠n: n·∫øu kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi th√¨ v·∫´n n√™n tr·∫£ √≠t nh·∫•t 1 d√≤ng
    
#     chat_ui = [(str(u), str(a)) for u, a in chat_ui]
#     if not chat_ui:
#         chat_ui = [(user_input, "‚ö†Ô∏è LLM kh√¥ng tr·∫£ l·ªùi ƒë∆∞·ª£c.")]
#     print(">>> Chat UI:", chat_ui)
#     print(">>> Type:", type(chat_ui), "Length:", len(chat_ui))
#     print(">>> Sample:", chat_ui[:1])


#     return gr.update(value=chat_ui, visible=True), state
    # return chat_ui, state


    # ****************************************************************

# def llm_gr_chat(user_msg, state):
#     if not isinstance(state, dict):
#         state = {"qwen_history": [], "last_image": None, "llm_history": []}

#     history = state["llm_history"]

#     # G·ªçi model th·∫≠t ƒë·ªÉ tr·∫£ l·ªùi
#     reply, updated_history = llm_chat_infer(user_msg, history)

#     state["llm_history"] = updated_history

#     # Chuy·ªÉn OpenAI-style history ‚Üí Gradio-compatible
#     chat_ui = []
#     for i in range(1, len(updated_history), 2):
#         u = updated_history[i - 1].get("content", "")
#         a = updated_history[i].get("content", "")
#         chat_ui.append([u, a])

#     if not chat_ui:
#         chat_ui = [[user_msg, "‚ö†Ô∏è LLM kh√¥ng tr·∫£ l·ªùi ƒë∆∞·ª£c."]]

#     return gr.update(value=chat_ui, visible=True), state
# state = gr.State({"llm_history": [], "full_history_ui": ""})
# import re

# def strip_think_tags(text):
#     # X√≥a ƒëo·∫°n n·∫±m trong <think>...</think>
#     return re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL).strip()

# def llm_gr_chat_1(user_msg, state):
#     history = state["llm_history"]

#     full_reply, updated_history = llm_chat_infer(user_msg, history)
#     # updated_history.append({"role": "user", "content": user_msg})
#     # updated_history.append({"role": "assistant", "content": full_reply})
#     state["llm_history"] = updated_history

#     # short_reply = truncate_response(full_reply, max_lines=4)
#     full_cleaned = strip_think_tags(full_reply)
    
#     # T·∫°o UI r√∫t g·ªçn cho Chatbot
#     chat_ui = []
#     for i in range(1, len(updated_history), 2):
#         u = updated_history[i-1]["content"]
#         a = updated_history[i]["content"]
#         # a_short = truncate_response(a, max_lines=4)
#         chat_ui.append([u, a])

#     # G·ªôp full ƒë·ªÉ xem l·∫°i
#     full_history_ui = ""
#     for i in range(1, len(updated_history), 2):
#         q = updated_history[i-1]["content"]
#         a = updated_history[i]["content"]
#         full_history_ui += f"üë®‚Äçüíª User: {q}\n\n ü§ñ Clara: {a}\n\n ----------------------------------------\n\n"

#     state["full_history_ui"] = full_history_ui
#     full_history_cleared = strip_think_tags(full_history_ui)

#     return gr.update(value=full_cleaned), gr.update(value=full_history_cleared), state, "" #gr.update(value=chat_ui), 

# ****************************************************************




# def chat_with_llm(user_msg, state):
#     if "llm_history" not in state or not state["llm_history"]:
#         return state.get("llm_history", []), state

#     state["llm_history"].append({"role": "user", "content": user_msg})
    
#     text_prompt = tokenizer.apply_chat_template(
#         state["llm_history"], tokenize=False, add_generation_prompt=True, enable_thinking=True
#     )
    
#     inputs = tokenizer(text_prompt, return_tensors="pt", padding=True, truncation=True, max_length=2048).to("cuda")
#     output_ids = model.generate(input_ids=inputs.input_ids, attention_mask=inputs.attention_mask, max_new_tokens=1024)
#     response = tokenizer.decode(output_ids[0], skip_special_tokens=True)
#     response = response.split("<|assistant|>\n")[-1].strip()

#     state["llm_history"].append({"role": "assistant", "content": response})
    
#     messages = [(msg["content"], None) if msg["role"] == "user" else (None, msg["content"]) for msg in state["llm_history"] if msg["role"] != "system"]
#     return messages, state

# demo.launch(share=True)

# with gr.Blocks() as demo:
#     gr.Markdown("## Pythera Clara (Clinical Language Analytics and Reasoning AI)")

#     shared_image = gr.State(None)   
#     # state_clara = gr.State({"qwen_history": [], "last_image": None})
#     state_clara = gr.State({"qwen_history": [], "last_image": None, "llm_history": []})

#     state_gemini = gr.State([])
#     state_gpt = gr.State([])

#     with gr.Row():
#         # ==== C·ªòT TR√ÅI ====
#         with gr.Column(scale=1):
#             sex_input = gr.Dropdown(choices=["Nam", "N·ªØ"], label="Gi·ªõi t√≠nh")
#             age_input = gr.Textbox(label="Tu·ªïi", placeholder="VD: 45")
#             view_input = gr.Dropdown(choices=["PA", "Lateral", "AP"], label="G√≥c ch·ª•p", value="PA")
#             image_input = gr.Image(type="pil", label="Upload ·∫£nh X-quang")

#             # ==== TEST CASE ====
#             examples = [
#                 ["image_test/test.png", "Nam", "81", "PA"],
#                 ["image_test/test2.png", "N·ªØ", "75", "AP"],
#                 ["image_test/test1.png", "Nam", "62", "PA"],
#                 ["image_test/test3.png", "Nam", "48", "PA"],
#                 ["image_test/test4.png", "Nam", "84", "PA"],
#                 ["image_test/test_1.png", "Nam", "53", "PA"],
#                 ["image_test/test_2.png", "Nam", "35", "PA"],
#                 ["image_test/test_3.png", "Nam", "79", "PA"],
#                 ["image_test/test_4.png", "N·ªØ", "33", "PA"],
#                 # ["/home/tiennv/phucth/medical/data_test/data/testcase_medical/image_test/test_5.png", "Nam", "55", "Lateral"],

#             ]

#             gr.Examples(
#                 examples=examples,
#                 inputs=[image_input, sex_input, age_input, view_input],
#                 label="üß™ Ch·ªçn Test Case m·∫´u"
#             )

#             # ==== RESET ====
#             reset_btn = gr.Button("üóëÔ∏è Reset t·∫•t c·∫£", variant="stop")

#         # ==== C·ªòT PH·∫¢I ====
#         with gr.Column(scale=2):
#             with gr.Tabs():
#                 with gr.Tab("Clara"):
#                     clara_model_selector = gr.Dropdown(
#                         choices=["Clara-custom"],  # , "Clara-mini"
#                         value="Clara",
#                         label="model Clara"
#                     )
#                     submit_c = gr.Button("Ph√¢n t√≠ch v·ªõi Clara")
#                     # chatbot_c = gr.Chatbot(label="K·∫øt qu·∫£ t·ª´ Clara", height=500)
#                     # # chatbot_llm = gr.Chatbot(label="Tr·ª£ l√Ω b√°c sƒ© (LLM)", height=400)
#                     # chatbot_ui = gr.Chatbot(label="Tr·ª£ l√Ω ch·∫©n ƒëo√°n t·ªïng h·ª£p", height=600)

#                     # llm_input = gr.Textbox(label="Nh·∫≠p c√¢u h·ªèi", placeholder="V√≠ d·ª•: B·ªánh nh√¢n c√≥ c·∫ßn nh·∫≠p vi·ªán kh√¥ng?")
#                     # # llm_send = gr.Button("G·ª≠i c√¢u h·ªèi ƒë·∫øn LLM")
#                     # # llm_input = gr.Textbox(label="Nh·∫≠p c√¢u h·ªèi", placeholder="V√≠ d·ª•: ...")
                    
#                     # llm_send = gr.Button("G·ª≠i c√¢u h·ªèi ƒë·∫øn LLM")
#                     chatbot_ui = gr.Chatbot(label="Tr·ª£ l√Ω", height=400, render_markdown=True)
#                     # chatbot_llm = gr.Chatbot(label="üí¨ Tr·ª£ l√Ω LLM", height=400, render_markdown=True)
#                     llm_input = gr.Textbox(label="Nh·∫≠p c√¢u h·ªèi")
#                     # llm_full_reply = gr.Textbox(label="C√¢u tr·∫£ l·ªùi chi ti·∫øt", lines=10, interactive=False)
#                     llm_full_reply = gr.Markdown(label="üìÑ C√¢u tr·∫£ l·ªùi chi ti·∫øt")

#                     # llm_full_history = gr.Textbox(label="üßæ L·ªãch s·ª≠ ƒë·∫ßy ƒë·ªß", lines=20, interactive=False)
#                     with gr.Accordion("üßæ Xem l·ªãch s·ª≠ ƒë·∫ßy ƒë·ªß", open=False):
#                         llm_full_history = gr.Markdown(visible=True)  # Ho·∫∑c Textbox n·∫øu b·∫°n kh√¥ng d√πng markdown

                    
#                     # full_textbox = gr.Textbox(label="N·ªôi dung chi ti·∫øt")
#                     state_clara = gr.State({"llm_history": []})
#                     submit_c.click(
#                         fn=safe_clara_infer,
#                         inputs=[shared_image, clara_model_selector, state_clara, sex_input, age_input, view_input],
#                         outputs=[chatbot_ui, state_clara]
#                     )
#                     # llm_input.submit(
#                     #     fn=llm_gr_chat,
#                     #     inputs=[llm_input, state_clara],
#                     #     outputs=[chatbot_ui, state_clara]
#                     # )


#                     # G·ª≠i khi nh·∫•n n√∫t
#                     # llm_send.click(fn=llm_gr_chat, inputs=[llm_input, state_clara], outputs=[chatbot_ui, state_clara])
#                     # G·ª≠i khi Enter


#                     # llm_input.submit(fn=llm_gr_chat_1, inputs=[llm_input, state_clara], outputs=[chatbot_ui,full_textbox, state_clara])

#                     llm_input.submit(fn=llm_gr_chat_1, 
#                         inputs=[llm_input, state_clara], 
#                         outputs=[llm_full_reply, llm_full_history, state_clara, llm_input]) #chatbot_llm, 


#                 # with gr.Tab("Gemini"):
#                 #     submit_gemini = gr.Button("Ph√¢n t√≠ch v·ªõi Gemini")
#                 #     output_gemini = gr.Textbox(label="K·∫øt qu·∫£ t·ª´ Gemini", lines=10)

#                 #     submit_gemini.click(
#                 #         fn=safe_gemini_infer,
#                 #         inputs=[shared_image, sex_input, age_input, view_input, state_gemini],
#                 #         outputs=[output_gemini, state_gemini]
#                 #     )

#                 # with gr.Tab("GPT-4o"):
#                 #     submit_gpt = gr.Button("Ph√¢n t√≠ch v·ªõi GPT-4o")
#                 #     output_gpt = gr.Textbox(label="K·∫øt qu·∫£ t·ª´ GPT-4o", lines=10)

#                 #     submit_gpt.click(
#                 #         fn=safe_gpt_infer,
#                 #         inputs=[shared_image, sex_input, age_input, view_input, state_gpt],
#                 #         outputs=[output_gpt, state_gpt]
#                 #     )

#     # === C·∫¨P NH·∫¨T ·∫¢NH D√ôNG CHUNG ===
#     def update_shared_image(image):
#         return image

#     image_input.change(fn=update_shared_image, inputs=image_input, outputs=shared_image)

#     # === RESET TO√ÄN B·ªò ===
#     # def reset_all():
#     #     return (
#     #         None,       # image_input
#     #         None,       # sex_input
#     #         "",         # age_input
#     #         "PA",       # view_input
#     #         {"llm_history": []},  # state_clara
#     #         {"gemini_history": []},  # state_gemini
#     #         {"gpt_history": []},     # state_gpt
#     #         [],         # chatbot_ui
#     #         "",         # output_gemini
#     #         "",         # output_gpt
#     #         "",         # llm_full_reply
#     #         ""          # llm_full_history
#     #     )
#     # reset_btn.click(
#     #     fn=reset_all,
#     #     inputs=[],
#     #     outputs=[
#     #         image_input,
#     #         sex_input,
#     #         age_input,
#     #         view_input,
#     #         state_clara,
#     #         state_gemini,
#     #         state_gpt,
#     #         chatbot_ui,
#     #         output_gemini,
#     #         output_gpt,
#     #         llm_full_reply,
#     #         llm_full_history
#     #     ]
#     # )
#     def reset_all():
#         return (
#             None,        # image_input
#             None,        # sex_input
#             "",          # age_input
#             "PA",        # view_input
#             {"llm_history": []},  # state_clara
#             [],          # state_gemini
#             [],          # state_gpt
#             [],          # chatbot_ui (Tr·ª£ l√Ω ch·∫©n ƒëo√°n)
#             "",          # llm_full_reply
#             "",          # llm_full_history
#             # "",          # output_gemini
#             # "",          # output_gpt
#         )




#     reset_btn.click(
#         fn=reset_all,
#         inputs=[],
#         outputs=[
#             image_input,       # 1
#             sex_input,         # 2
#             age_input,         # 3
#             view_input,        # 4
#             state_clara,       # 5
#             state_gemini,      # 6
#             state_gpt,         # 7
#             chatbot_ui,        # 8 (n·∫øu b·∫°n d√πng chatbot ch·∫©n ƒëo√°n t·ªïng h·ª£p)
#             llm_full_reply,    # 9
#             llm_full_history,  # 10
#             # output_gemini,     # 11
#             # output_gpt         # 12
#         ]
#     )




# # demo.launch(share=True)
# demo.launch(
#     share=True,
#     allowed_paths=["/home/clara/phucth/code/project/Medical_CLARA/infer/demo_clara/image_test"]
# )


import gradio as gr

with gr.Blocks() as demo:
    gr.Markdown("## Pythera Clara (Clinical Language Analytics and Reasoning AI)")

    # ==== State d√πng chung ====
    shared_image = gr.State(None)
    state_clara = gr.State({"qwen_history": [], "last_image": None, "llm_history": []})
    state_gemini = gr.State([])
    state_gpt = gr.State([])

    with gr.Row():
        # ==== C·ªòT TR√ÅI: Form upload + th√¥ng tin b·ªánh nh√¢n ====
        with gr.Column(scale=1):
            sex_input = gr.Dropdown(choices=["Nam", "N·ªØ"], label="Gi·ªõi t√≠nh")
            age_input = gr.Textbox(label="Tu·ªïi", placeholder="VD: 45")
            view_input = gr.Dropdown(choices=["PA", "Lateral", "AP"], label="G√≥c ch·ª•p", value="PA")
            image_input = gr.Image(type="pil", label="Upload ·∫£nh X-quang")

            # ==== Test case m·∫´u ====
            examples = [
                ["image_test/test.png", "Nam", "81", "PA"],
                ["image_test/test2.png", "N·ªØ", "75", "AP"],
                ["image_test/test1.png", "Nam", "62", "PA"],
                ["image_test/test3.png", "Nam", "48", "PA"],
                ["image_test/test4.png", "Nam", "84", "PA"],
                ["image_test/test_1.png", "Nam", "53", "PA"],
                ["image_test/test_2.png", "Nam", "35", "PA"],
                ["image_test/test_3.png", "Nam", "79", "PA"],
                ["image_test/test_4.png", "N·ªØ", "33", "PA"],
            ]

            gr.Examples(
                examples=examples,
                inputs=[image_input, sex_input, age_input, view_input],
                label="üß™ Ch·ªçn Test Case m·∫´u"
            )

        # ==== C·ªòT PH·∫¢I: Clara Tab ====
        with gr.Column(scale=2):
            with gr.Tabs():
                with gr.Tab("Clara"):
                    # ==== Dropdown ch·ªçn model ====
                    clara_model_selector = gr.Dropdown(
                        choices=["Clara"],
                        value="Clara",   # ch·ªçn s·∫µn
                        label="Model Clara"
                    )

                    # ==== Row n√∫t Submit + Reset n·∫±m c·∫°nh nhau ====
                    with gr.Row():
                        submit_c = gr.Button("Ph√¢n t√≠ch v·ªõi Clara")
                        reset_btn = gr.Button("üóëÔ∏è Reset t·∫•t c·∫£", variant="stop")

                    # ==== Chatbot hi·ªÉn th·ªã k·∫øt qu·∫£ ====
                    chatbot_ui = gr.Chatbot(label="Tr·ª£ l√Ω", height=400, render_markdown=True)
                    llm_input = gr.Textbox(label="Nh·∫≠p c√¢u h·ªèi")
                    llm_full_reply = gr.Markdown(label="üìÑ C√¢u tr·∫£ l·ªùi chi ti·∫øt")
                    with gr.Accordion("üßæ Xem l·ªãch s·ª≠ ƒë·∫ßy ƒë·ªß", open=False):
                        llm_full_history = gr.Markdown(visible=True)

                    state_clara = gr.State({"llm_history": []})

                    # ==== Submit click ====
                    submit_c.click(
                        fn=safe_clara_infer,
                        inputs=[shared_image, clara_model_selector, state_clara, sex_input, age_input, view_input],
                        outputs=[chatbot_ui, state_clara]
                    )

                    # ==== Reset click ====
                    def reset_all():
                        return (
                            None,        # image_input
                            None,        # sex_input
                            "",          # age_input
                            "PA",        # view_input
                            {"llm_history": []},  # state_clara
                            # [],          # state_gemini
                            # [],          # state_gpt
                            # [],          # chatbot_ui
                            # "",          # llm_full_reply
                            # "",          # llm_full_history
                        )

                    reset_btn.click(
                        fn=reset_all,
                        inputs=[],
                        outputs=[
                            image_input,
                            sex_input,
                            age_input,
                            view_input,
                            state_clara,
                            # state_gemini,
                            # state_gpt,
                            # chatbot_ui,
                            # llm_full_reply,
                            # llm_full_history
                        ]
                    )

                    # # ==== LLM input submit (Enter) ====
                    # llm_input.submit(
                    #     fn=llm_gr_chat_1,
                    #     inputs=[llm_input, state_clara],
                    #     outputs=[llm_full_reply, llm_full_history, state_clara, llm_input]
                    # )

    # === C·∫≠p nh·∫≠t ·∫£nh d√πng chung ===
    def update_shared_image(image):
        return image

    image_input.change(fn=update_shared_image, inputs=image_input, outputs=shared_image)

# === Launch demo ===
demo.launch(
    share=True,
    allowed_paths=["/home/clara/phucth/code/project/Medical_CLARA/infer/demo_clara/image_test"]
)
