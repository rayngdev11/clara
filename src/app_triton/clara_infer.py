import gradio as gr
import torch
from transformers import AutoModelForVision2Seq, AutoProcessor
from PIL import Image
from io import BytesIO
import base64
from huggingface_hub import login
import torch
torch.cuda.empty_cache()  # gi·∫£i ph√≥ng VRAM cache, kh√¥ng x√≥a tensors ƒëang d√πng

# ƒêƒÉng nh·∫≠p Hugging Face (n·∫øu c·∫ßn)
login(token="")

# Thi·∫øt l·∫≠p thi·∫øt b·ªã
device = "cuda" if torch.cuda.is_available() else "cpu"
# device = "cpu"  # Ch·∫°y tr√™n CPU n·∫øu kh√¥ng c√≥ GPU
# Danh s√°ch model c√≥ th·ªÉ ch·ªçn
MODEL_INFOS = {
    "Clara": {
        "model_id": "THP2903/Qwen2-VL-7B-multi-137k_full_maed",
        "processor_id": "THP2903/Qwen2vl_7b_instruct_medical_multiturn_full"
        # "model_id": "THP2903/Qwen2vl_instruct_medical_2",
        # "processor_id": "THP2903/Qwen2vl_instruct_medical_2"
    },
    "Clara-mini": {
        "model_id": "THP2903/Qwen2vl_7b_instruct_medical_multiturn_full",
        "processor_id": "THP2903/Qwen2vl_7b_instruct_medical_multiturn_full"
    }
}

# # Bi·∫øn to√†n c·ª•c l∆∞u model hi·ªán t·∫°i
# current_model_name = None
# current_model = None
# current_processor = None
# prev_question_text = None
# turn_count = 0

# # Resize ·∫£nh ƒë·ªÉ inference
# def resize_image(image: Image.Image, max_size: int = 1408) -> Image.Image:
#     if image is None:
#         return None
#     w, h = image.size
#     if max(w, h) <= max_size:
#         return image
#     scale = max_size / max(w, h)
#     new_w, new_h = int(w * scale), int(h * scale)
#     return image.resize((new_w, new_h), resample=Image.BILINEAR)

# # T√°ch ·∫£nh t·ª´ l·ªãch s·ª≠ h·ªôi tho·∫°i
# def process_vision_info(chat_history):
#     images = []
#     for message in chat_history:
#         for content in message["content"]:
#             if content["type"] == "image":
#                 images.append(content["image"])
#     return images if images else None, None

# # H√†m ch√≠nh ch·∫°y model Qwen
# def run_qwen_model(user_text, user_img, model_name, history, max_new_tokens=512, temperature=0.7, top_p=0.9, top_k=50):
#     global current_model, current_processor, current_model_name
#     global prev_question_text, turn_count

#     chat_history_qwen = history.get("qwen_history", [])
#     display_history = history.get("display_history", [])

#     # Load model n·∫øu ch∆∞a c√≥ ho·∫∑c kh√°c model c≈©
#     if model_name != current_model_name:
#         if current_model is not None:
#             del current_model
#             torch.cuda.empty_cache()
#         current_model = AutoModelForVision2Seq.from_pretrained(
#             MODEL_INFOS[model_name]["model_id"],
#             torch_dtype=torch.bfloat16,
#             low_cpu_mem_usage=True,
#             device_map="auto"
#         )
#         current_processor = AutoProcessor.from_pretrained(MODEL_INFOS[model_name]["processor_id"])
#         current_model_name = model_name

#     # ƒê·∫øm s·ªë l∆∞·ª£t h·ªôi tho·∫°i
#     if prev_question_text is not None and user_text.strip() != prev_question_text.strip():
#         turn_count += 1
#     elif prev_question_text is None:
#         turn_count = 1
#     prev_question_text = user_text.strip()

#     # G·∫Øn system message n·∫øu l√† l∆∞·ª£t ƒë·∫ßu
#     if not chat_history_qwen:
#         chat_history_qwen.append({
#             "role": "system",
#             "content": [{"type": "text", "text": "B·∫°n l√† m·ªôt tr·ª£ l√Ω b√°c sƒ©. Tr·∫£ l·ªùi ch√≠nh x√°c, d·ªÖ hi·ªÉu."}]
#         })

#     # Chu·∫©n b·ªã input
#     user_content = [{"type": "text", "text": user_text}]
#     if user_img is not None:
#         user_img = resize_image(user_img, max_size=512)
#         user_content.append({"type": "image", "image": user_img})
#     chat_history_qwen.append({"role": "user", "content": user_content})

#     # T·∫°o prompt v√† x·ª≠ l√Ω ·∫£nh
#     text_prompt = current_processor.apply_chat_template(chat_history_qwen, tokenize=False, add_generation_prompt=True)
#     images, _ = process_vision_info(chat_history_qwen)
#     inputs = current_processor(text=[text_prompt], images=images, return_tensors="pt", padding=True).to(device)

#     with torch.no_grad():
#         output_ids = current_model.generate(
#             **inputs,
#             max_new_tokens=int(max_new_tokens),
#             temperature=float(temperature),
#             top_p=float(top_p),
#             top_k=int(top_k),
#             eos_token_id=current_processor.tokenizer.eos_token_id
#         )

#     input_len = inputs.input_ids.shape[1]
#     generated_ids = output_ids[0][input_len:]
#     output_text = current_processor.decode(generated_ids, skip_special_tokens=True)

#     # Tr·∫£ l·ªùi t·ª´ model
#     chat_history_qwen.append({
#         "role": "assistant",
#         "content": [{"type": "text", "text": output_text}]
#     })

#     # Ch·ªâ hi·ªÉn th·ªã text (‚ùå KH√îNG hi·ªÉn th·ªã ·∫£nh)
#     user_message = user_text
#     display_history.append((user_message, output_text))

#     # C·∫≠p nh·∫≠t l·ªãch s·ª≠
#     history["qwen_history"] = chat_history_qwen
#     history["display_history"] = display_history

#     return display_history, history

# # ===== Gradio Interface =====
# with gr.Blocks() as demo:
#     gr.Markdown("## ü©ª Tr·ª£ l√Ω X-ray AI - Qwen Medical")

#     with gr.Row():
#         model_selector = gr.Dropdown(choices=list(MODEL_INFOS.keys()), value="Clara", label="Ch·ªçn model")
#         submit_btn = gr.Button("G·ª≠i")

#     with gr.Row():
#         user_text = gr.Textbox(label="Nh·∫≠p c√¢u h·ªèi", placeholder="V√≠ d·ª•: ƒê√¢y l√† ·∫£nh X-quang tim ph·ªïi, b·ªánh nh√¢n b·ªã g√¨?")
#         image_input = gr.Image(type="pil", label="Upload ·∫£nh X-ray")

#     chatbot_display = gr.Chatbot(label="K·∫øt qu·∫£ t∆∞ v·∫•n", height=600)
#     state = gr.State({"qwen_history": [], "display_history": []})

#     submit_btn.click(
#         fn=run_qwen_model,
#         inputs=[user_text, image_input, model_selector, state],
#         outputs=[chatbot_display, state]
#     )

# demo.launch()


current_model_name = None
current_model = None
current_processor = None

def resize_image(image: Image.Image, max_size: int = 448) -> Image.Image:
    if image is None:
        return None
    w, h = image.size
    if max(w, h) <= max_size:
        return image
    scale = max_size / max(w, h)
    new_w, new_h = int(w * scale), int(h * scale)
    return image.resize((new_w, new_h), resample=Image.BILINEAR)

def process_vision_info(chat_history):
    images = []
    for message in chat_history:
        for content in message["content"]:
            if content["type"] == "image":
                images.append(content["image"])
    return images if images else None, None
from transformers import AutoModelForVision2Seq, AutoProcessor, AutoConfig
from transformers import Qwen2VLProcessor, Qwen2VLForConditionalGeneration
# G·ªçi model m·ªôt l∆∞·ª£t    _ (  CODE ƒê√öNG KO ƒê∆Ø·ª¢C CH·ªàNH S·ª¨A TH√äM)
def run_qwen_model(user_text, user_img, model_name, history, max_new_tokens=512, temperature=0.7, top_p=1.0, top_k=30):  # 512 -> 256     0.7 0.9 50 
    global current_model, current_processor, current_model_name

    chat_history_qwen = history.get("qwen_history", []) 

    # if model_name != current_model_name:
    #     if current_model is not None:
    #         del current_model
    #         torch.cuda.empty_cache()
    #     current_model = AutoModelForVision2Seq.from_pretrained(
    #         MODEL_INFOS[model_name]["model_id"],
    #         torch_dtype=torch.bfloat16,  # torch_dtype=torch.float16

    #         low_cpu_mem_usage=True,
    #         device_map="auto"   # sequential
    #     )
    #     current_processor = AutoProcessor.from_pretrained(MODEL_INFOS[model_name]["processor_id"])
    #     current_model_name = model_name
    

    if model_name != current_model_name:
        if current_model is not None:
            del current_model
            torch.cuda.empty_cache()

        current_model = Qwen2VLForConditionalGeneration.from_pretrained(
            MODEL_INFOS[model_name]["model_id"],
            torch_dtype=torch.bfloat16,
            device_map="auto"
            # device_map="cpu"  
        )

        current_processor = Qwen2VLProcessor.from_pretrained(
            MODEL_INFOS[model_name]["processor_id"],
            use_fast=True
        )

        current_model_name = model_name

    # Th√™m system message n·∫øu l·∫ßn ƒë·∫ßu
    if not chat_history_qwen:
        chat_history_qwen.append({
            "role": "system",
            "content": [{"type": "text", "text": "B·∫°n l√† m·ªôt tr·ª£ l√Ω b√°c sƒ©. Tr·∫£ l·ªùi ch√≠nh x√°c, d·ªÖ hi·ªÉu."}]
        })

    # Chu·∫©n b·ªã n·ªôi dung ng∆∞·ªùi d√πng
    user_content = [{"type": "text", "text": user_text}]
    if user_img is not None:
        user_img = resize_image(user_img, max_size=448)
        user_content.append({"type": "image", "image": user_img})
    chat_history_qwen.append({"role": "user", "content": user_content})

    text_prompt = current_processor.apply_chat_template(chat_history_qwen, tokenize=False, add_generation_prompt=True)
    images, _ = process_vision_info(chat_history_qwen)
    inputs = current_processor(text=[text_prompt], images=images, return_tensors="pt", padding=True).to(device)

    with torch.no_grad():
        output_ids = current_model.generate(
            **inputs,
            max_new_tokens=int(max_new_tokens),
            temperature=float(temperature),
            top_p=float(top_p),
            top_k=int(top_k),
            eos_token_id=current_processor.tokenizer.eos_token_id
        )

    input_len = inputs.input_ids.shape[1]
    generated_ids = output_ids[0][input_len:]
    output_text = current_processor.decode(generated_ids, skip_special_tokens=True)

    # Ghi l·∫°i ph·∫£n h·ªìi
    chat_history_qwen.append({
        "role": "assistant",
        "content": [{"type": "text", "text": output_text}]
    })

    # C·∫≠p nh·∫≠t l·ªãch s·ª≠
    history["qwen_history"] = chat_history_qwen
    return output_text, history


# def run_qwen_model(
#     user_text,
#     user_img,
#     model_name,
#     history,
#     max_new_tokens=512,
#     temperature=0.9,
#     top_p=1.0,
#     top_k=0,
#     deterministic=False  # ‚úÖ th√™m t√πy ch·ªçn c·ªë ƒë·ªãnh output
# ):
#     global current_model, current_processor, current_model_name

#     chat_history_qwen = history.get("qwen_history", [])

#     # T·∫£i model n·∫øu c·∫ßn
#     if model_name != current_model_name:
#         if current_model is not None:
#             del current_model
#             torch.cuda.empty_cache()

#         current_model = Qwen2VLForConditionalGeneration.from_pretrained(
#             MODEL_INFOS[model_name]["model_id"],
#             torch_dtype=torch.bfloat16,
#             device_map="auto"
#         )

#         current_processor = Qwen2VLProcessor.from_pretrained(
#             MODEL_INFOS[model_name]["processor_id"],
#             use_fast=True
#         )

#         current_model_name = model_name

#     # Th√™m system prompt n·∫øu l·∫ßn ƒë·∫ßu
#     if not chat_history_qwen:
#         chat_history_qwen.append({
#             "role": "system",
#             "content": [{"type": "text", "text": "B·∫°n l√† m·ªôt tr·ª£ l√Ω b√°c sƒ©. Tr·∫£ l·ªùi ch√≠nh x√°c, d·ªÖ hi·ªÉu."}]
#         })

#     # T·∫°o user message
#     user_content = [{"type": "text", "text": user_text}]
#     if user_img is not None:
#         user_img = resize_image(user_img, max_size=448)
#         user_content.append({"type": "image", "image": user_img})

#     chat_history_qwen.append({"role": "user", "content": user_content})

#     # T·∫°o prompt ƒë·∫ßu v√†o
#     text_prompt = current_processor.apply_chat_template(
#         chat_history_qwen,
#         tokenize=False,
#         add_generation_prompt=True
#     )
#     images, _ = process_vision_info(chat_history_qwen)

#     inputs = current_processor(
#         text=[text_prompt],
#         images=images,
#         return_tensors="pt",
#         padding=True
#     ).to(device)

#     # ‚úÖ C·ªë ƒë·ªãnh seed n·∫øu y√™u c·∫ßu output ·ªïn ƒë·ªãnh
#     if deterministic:
#         torch.manual_seed(42)

#     # ‚úÖ T√πy ch·ªânh decoding strategy
#     gen_kwargs = {
#         "max_new_tokens": int(max_new_tokens),
#         "eos_token_id": current_processor.tokenizer.eos_token_id,
#     }

#     if deterministic:
#         gen_kwargs.update({
#             "do_sample": False,  # d√πng greedy decoding ƒë·ªÉ c·ªë ƒë·ªãnh output
#         })
#     else:
#         gen_kwargs.update({
#             "do_sample": True,
#             "temperature": float(temperature),
#             "top_p": float(top_p),
#             "top_k": int(top_k),
#         })

#     # Generate
#     with torch.no_grad():
#         output_ids = current_model.generate(**inputs, **gen_kwargs)

#     input_len = inputs.input_ids.shape[1]
#     generated_ids = output_ids[0][input_len:]
#     output_text = current_processor.decode(generated_ids, skip_special_tokens=True)

#     # L∆∞u k·∫øt qu·∫£ v√†o history
#     chat_history_qwen.append({
#         "role": "assistant",
#         "content": [{"type": "text", "text": output_text}]
#     })

#     history["qwen_history"] = chat_history_qwen
#     return output_text, history


def to_bullet_list(text):
    lines = text.strip().split("\n")
    items = "\n".join(f"‚Ä¢ {line.lstrip('-').strip()}" for line in lines if line.strip())
    return items

def multiturn_infer(image, model_name, state_dict, sex, age, view):
    # ‚úÖ S·ª≠a l·ªói: n·∫øu reset th√¨ state_dict c√≥ th·ªÉ l√† list ‚Üí √©p v·ªÅ dict
    if not isinstance(state_dict, dict):
        state_dict = {}

    # T·∫°o prompt ƒë·ªông
    view_full = f"X-ray {view}" if view else "X-ray"
    sex_text = f"b·ªánh nh√¢n {sex.lower()}" if sex else "b·ªánh nh√¢n"
    age_text = f", {age} tu·ªïi" if age else ""

    # Prompt 1: Findings
    prompt_findings = f"·∫¢nh ch·ª•p {view_full} {sex_text}{age_text}. Cho bi·∫øt b·ªánh nh√¢n b·ªã g√¨?"
    prompt_impression = "K·∫øt lu·∫≠n t·ª´ th√¥ng tin tr√™n b·ªánh nh√¢n b·ªã g√¨?"

    # X·ª≠ l√Ω ·∫£nh
    last_image = state_dict.get("last_image", None)
    if image is None:
        if last_image is None:
            warning = "‚ö†Ô∏è B·∫°n c·∫ßn upload ·∫£nh X-quang ·ªü l·∫ßn ƒë·∫ßu."
            return [(warning, "")], state_dict, None
        else:
            image_to_use = last_image
    else:
        image_to_use = image
        state_dict["last_image"] = image

    findings_response, state_dict = run_qwen_model(prompt_findings, image_to_use, model_name, state_dict)
    impression_response, state_dict = run_qwen_model(prompt_impression, image_to_use, model_name, state_dict)
    final_response = f"""üñºÔ∏è **H√¨nh ·∫£nh cho th·∫•y:**\n{to_bullet_list(findings_response)}\n\nüîç **Ch·∫©n ƒëo√°n:**\n{to_bullet_list(impression_response)}"""

    return [(prompt_findings, final_response)], state_dict, None

import base64
import numpy as np
from io import BytesIO
from PIL import Image
from tritonclient.http import InferenceServerClient, InferInput

# =========================================================
# Helper: Encode image to base64
# =========================================================
def encode_image_to_b64(image_pil):
    buffered = BytesIO()
    image_pil.save(buffered, format="JPEG")
    img_bytes = buffered.getvalue()
    return base64.b64encode(img_bytes).decode("utf-8")


# =========================================================
# Helper: Triton inference
# =========================================================
def triton_infer(model_name, text_list, image_b64_list=None, url="localhost:8000"):
    """
    Robust Triton infer helper:
    - G·ªçi client
    - Ghi log outputs tr·∫£ v·ªÅ ƒë·ªÉ debug t√™n output
    - Th·ª≠ ƒë·ªçc `text_output` tr∆∞·ªõc v√¨ code test c·ªßa b·∫°n d√πng 'text_output'
    - N·∫øu kh√¥ng c√≥, in ra response.get_response().outputs ƒë·ªÉ b·∫°n bi·∫øt t√™n ƒë√∫ng
    """
    client = InferenceServerClient(url=url)

    # TEXT INPUT
    text_arr = np.array([t.encode("utf-8") for t in text_list], dtype=object)
    text_input = InferInput("text_input", text_arr.shape, "BYTES")
    text_input.set_data_from_numpy(text_arr)

    # IMAGE INPUT
    if image_b64_list is None:
        image_b64_list = [""] * len(text_list)
    image_arr = np.array([i.encode("utf-8") for i in image_b64_list], dtype=object)
    image_input = InferInput("image_input", image_arr.shape, "BYTES")
    image_input.set_data_from_numpy(image_arr)

    # SEND
    response = client.infer(model_name=model_name, inputs=[text_input, image_input])

    # --- DEBUG: in ra c·∫•u tr√∫c outputs tr·∫£ v·ªÅ t·ª´ Triton ---
    try:
        raw_resp = response.get_response()
        print(">>> Triton response outputs:", raw_resp.outputs)
    except Exception:
        # some client versions may not expose get_response the same way
        pass

    # TH·ª¨ ƒë·ªçc theo t√™n m√† b·∫°n ƒë√£ d√πng trong script test: "text_output"
    outputs = response.as_numpy("text_output")
    if outputs is None:
        # Th·ª≠ m·ªôt v√†i t√™n ph·ªï bi·∫øn (fallback)
        for candidate in ["output_text", "OUTPUT_TEXT", "output_0", "OUTPUT", "response"]:
            try:
                outputs = response.as_numpy(candidate)
                if outputs is not None:
                    print(f">>> Using output name '{candidate}'")
                    break
            except Exception:
                outputs = None

    if outputs is None:
        # N·∫øu v·∫´n None: in debug th√™m v√† raise
        print("‚ùå Triton returned None for all tried output names. Inspect server logs & model config.pbtxt.")
        # in raw response bytes (if any)
        try:
            print("Raw response:", response.get_response())
        except Exception:
            pass
        raise RuntimeError("Triton returned None output ‚Äî check model output name or server error")

    # Decode outputs (bytes -> str)
    decoded = []
    for o in outputs:
        if isinstance(o, (bytes, bytearray)):
            decoded.append(o.decode("utf-8"))
        else:
            decoded.append(str(o))

    return decoded




# =========================================================
# MAIN FUNCTION: MULTI-TURN INFERENCE USING TRITON
# =========================================================
def multiturn_infer_triton(image, state_dict, sex, age, view):
    # Handle None state_dict
    if state_dict is None:
        state_dict = {}

    # USER INPUT FORMAT
    prompt_findings = f"H√¨nh X-ray {view} c·ªßa b·ªánh nh√¢n {sex}, {age} tu·ªïi. Cho bi·∫øt c√°c b·∫•t th∆∞·ªùng trong ·∫£nh."
    prompt_impression = "K·∫øt lu·∫≠n t·ª´ th√¥ng tin tr√™n b·ªánh nh√¢n b·ªã g√¨?"

    # ------------- TURN 1: FINDINGS (send image) -------------
    image_b64 = encode_image_to_b64(image)
    out1 = triton_infer(
        model_name="vlm",
        text_list=[prompt_findings],
        image_b64_list=[image_b64]
    )
    findings = out1[0]

    # ------------- TURN 2: IMPRESSION (no image) -------------
    out2 = triton_infer(
        model_name="vlm",
        text_list=[prompt_impression],
        image_b64_list=[""]  # no image for turn 2
    )
    impression = out2[0]

    # COMBINE FINAL
    final_response = (
        f"**Findings:** {findings}\n\n"
        f"**Impression:** {impression}"
    )

    # FORMAT CHAT HISTORY FOR GRADIO
    chat_history = [
        (prompt_findings, findings),
        (prompt_impression, impression),
    ]

    return chat_history, state_dict

"""
h√†m chu·∫©n v·ªÅ singleturn_infer
"""

def singleturn_infer(image, model_name, state_dict, sex, age, view):
    # N·∫øu reset -> √©p v·ªÅ dict
    if not isinstance(state_dict, dict):
        state_dict = {}

    # T·∫°o prompt duy nh·∫•t
    view_full = f"X-ray {view}" if view else "X-ray"
    sex_text = f"b·ªánh nh√¢n {sex.lower()}" if sex else "b·ªánh nh√¢n"
    age_text = f", {age} tu·ªïi" if age else ""

    prompt_findings = f"·∫¢nh ch·ª•p {view_full} {sex_text}{age_text}. Cho bi·∫øt b·ªánh nh√¢n b·ªã g√¨?"
    # prompt_findings = f"This is a {view_full} X-ray of a {sex_text}{age_text} patient. What is the diagnosis?"

    # D√πng ·∫£nh m·ªõi ho·∫∑c l·∫•y l·∫°i ·∫£nh c≈©
    last_image = state_dict.get("last_image", None)
    if image is None:
        if last_image is None:
            warning = "‚ö†Ô∏è B·∫°n c·∫ßn upload ·∫£nh X-quang ·ªü l·∫ßn ƒë·∫ßu."
            return [(warning, "")], state_dict, None
        else:
            image_to_use = last_image
    else:
        image_to_use = image
        state_dict["last_image"] = image

    # Ch·ªâ ch·∫°y 1 l∆∞·ª£t
    response_text, state_dict = run_qwen_model(prompt_findings, image_to_use, model_name, state_dict)
    final_response = f"""üîç **K·∫øt qu·∫£ ph√¢n t√≠ch:**\n{to_bullet_list(response_text)}"""

    return [(prompt_findings, final_response)], state_dict, None


"""
h√†m test singleturn_infer
"""
# def singleturn_infer(image, model_name, state_dict, sex, age, view,
#                      max_tokens=512, temperature=0.9, top_p=1, top_k=0):
#     # N·∫øu reset -> √©p v·ªÅ dict
#     if not isinstance(state_dict, dict):
#         state_dict = {}

#     # T·∫°o prompt duy nh·∫•t
#     view_full = f"X-ray {view}" if view else "X-ray"
#     sex_text = f"b·ªánh nh√¢n {sex.lower()}" if sex else "b·ªánh nh√¢n"
#     age_text = f", {age} tu·ªïi" if age else ""

#     prompt_findings = f"·∫¢nh ch·ª•p {view_full} {sex_text}{age_text}. Cho bi·∫øt b·ªánh nh√¢n b·ªã g√¨?"

#     # D√πng ·∫£nh m·ªõi ho·∫∑c l·∫•y l·∫°i ·∫£nh c≈©
#     last_image = state_dict.get("last_image", None)
#     if image is None:
#         if last_image is None:
#             warning = "‚ö†Ô∏è B·∫°n c·∫ßn upload ·∫£nh X-quang ·ªü l·∫ßn ƒë·∫ßu."
#             return [(warning, "")], state_dict, None
#         else:
#             image_to_use = last_image
#     else:
#         image_to_use = image
#         state_dict["last_image"] = image

#     # Ch·∫°y m√¥ h√¨nh Qwen
#     response_text, state_dict = run_qwen_model(
#         user_text=prompt_findings,
#         user_img=image_to_use,
#         model_name=model_name,
#         history=state_dict,
#         max_new_tokens=max_tokens,
#         temperature=temperature,
#         top_p=top_p,
#         top_k=top_k,
#         deterministic=True
#     )

#     final_response = f"""üîç **K·∫øt qu·∫£ ph√¢n t√≠ch:**\n{to_bullet_list(response_text)}"""
#     return [(prompt_findings, final_response)], state_dict, None


# ---------------------------------------------------------
# TEST HERE
# ---------------------------------------------------------
if __name__ == "__main__":
    # 1. Load local image (ƒë·ªïi path ·∫£nh c·ªßa b·∫°n ·ªü ƒë√¢y)
    # image_path = "/home/truongnn/phucth/image_test/image_bt.png"
    image_path = "/home/truongnn/phucth/image_test/image(4).png"

    image = Image.open(image_path).convert("RGB")

    # 2. Call multi-turn inference
    chat, _ = multiturn_infer_triton(
        image=image,
        state_dict={},
        sex="nam",
        age=60,
        view="PA"
    )

    # 3. Print result
    print("\n===== MULTITURN OUTPUT =====")
    for i, (user, bot) in enumerate(chat, 1):
        print(f"\n--- TURN {i} ---")
        print("USER:", user)
        print("MODEL:", bot)